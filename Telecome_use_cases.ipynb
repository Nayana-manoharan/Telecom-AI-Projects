{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVYvcehdf+zAZnGXz7bRQi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CALL CENTER OPTIMIZATION"
      ],
      "metadata": {
        "id": "dI_ijoi9xehR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAXFjcBqv3kh"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "# Load OpenAI API Key securely\n",
        "openai.api_key = \"\"  # Replace with your actual API key\n",
        "\n",
        "# Load dataset with caching\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    file_path = \"call_center_data_with_solutions.csv\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "data = load_data()\n",
        "\n",
        "# 🔹 Categorizing the query using OpenAI\n",
        "def categorize_query(query, data):\n",
        "    \"\"\"Uses OpenAI GPT to categorize the user query accurately.\"\"\"\n",
        "    if \"Call_Category\" not in data.columns:\n",
        "        return \"Unknown Category\"\n",
        "\n",
        "    unique_categories = data[\"Call_Category\"].dropna().unique().tolist()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI assistant for a call center. Your task is to categorize customer queries based on predefined categories.\n",
        "\n",
        "    **Customer Query:** \"{query}\"\n",
        "\n",
        "    **Available Categories:** {unique_categories}\n",
        "\n",
        "    Based on the query, select the most relevant category from the list.\n",
        "    Only return the category name as your final answer.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        category = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "        # Validate if the response is a known category\n",
        "        if category in unique_categories:\n",
        "            return category\n",
        "        else:\n",
        "            return \"Unknown Category\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error with OpenAI API: {str(e)}\"\n",
        "\n",
        "# 🔹 Finding the best dataset solution using OpenAI embeddings\n",
        "def get_best_dataset_solution(query, filtered_data):\n",
        "    \"\"\"Finds the most relevant solution from the dataset using OpenAI embeddings for similarity matching.\"\"\"\n",
        "    if \"Call_Transcript\" not in filtered_data.columns or \"Solution\" not in filtered_data.columns:\n",
        "        return \"Error: Missing necessary columns in dataset.\"\n",
        "\n",
        "    if filtered_data.empty:\n",
        "        return None  # No relevant category found\n",
        "\n",
        "    # Convert call transcripts to a list\n",
        "    call_transcripts = filtered_data[\"Call_Transcript\"].dropna().tolist()\n",
        "\n",
        "    # Get OpenAI embeddings for query and transcripts\n",
        "    try:\n",
        "        query_embedding = openai.Embedding.create(input=query, model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n",
        "        transcript_embeddings = openai.Embedding.create(input=call_transcripts, model=\"text-embedding-ada-002\")[\"data\"]\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarities = [np.dot(query_embedding, emb[\"embedding\"]) for emb in transcript_embeddings]\n",
        "\n",
        "        # Find the best match\n",
        "        best_match_idx = np.argmax(similarities)\n",
        "        best_solution = filtered_data.iloc[best_match_idx][\"Solution\"]\n",
        "\n",
        "        return best_solution\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error with OpenAI API: {str(e)}\"\n",
        "\n",
        "# 🔹 Finding the best solution using OpenAI GPT\n",
        "def get_ai_generated_solution(query, filtered_data):\n",
        "    \"\"\"Uses OpenAI to generate a solution when no exact match is found in the dataset.\"\"\"\n",
        "    if \"Call_Transcript\" not in filtered_data.columns or \"Solution\" not in filtered_data.columns:\n",
        "        return \"Error: Missing necessary columns in dataset.\"\n",
        "\n",
        "    # Create prompt with call transcripts and solutions\n",
        "    call_transcripts = \"\\n\".join(filtered_data[\"Call_Transcript\"].dropna().tolist()[:5])  # Limit to 5 for cost\n",
        "    solutions = \"\\n\".join(filtered_data[\"Solution\"].dropna().tolist()[:5])  # Limit to 5 for cost\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI call center assistant. A customer has asked the following question:\n",
        "\n",
        "    **User Query:** \"{query}\"\n",
        "\n",
        "    Below are previous call transcripts and their corresponding solutions:\n",
        "\n",
        "    **Call Transcripts:**\n",
        "    {call_transcripts}\n",
        "\n",
        "    **Solutions:**\n",
        "    {solutions}\n",
        "\n",
        "    Please find the best matching solution for the query. If no exact match exists, generate a helpful response.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        ai_solution = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "        return ai_solution\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error with OpenAI API: {str(e)}\"\n",
        "\n",
        "# 🔹 Streamlit UI\n",
        "st.title(\"AI-Powered Call Center Optimization\")\n",
        "\n",
        "user_query = st.text_area(\"Enter your query:\")\n",
        "\n",
        "if st.button(\"Find Solution\"):\n",
        "    if user_query.strip():\n",
        "        # Step 1: Categorize the query\n",
        "        category = categorize_query(user_query, data)\n",
        "        st.write(f\"**Identified Call Category:** {category}\")\n",
        "\n",
        "        # Step 2: Filter dataset by category\n",
        "        filtered_data = data[data[\"Call_Category\"] == category] if category != \"Unknown Category\" else data\n",
        "\n",
        "        # Step 3: Find best solution from dataset\n",
        "        dataset_solution = get_best_dataset_solution(user_query, filtered_data)\n",
        "\n",
        "        # Step 4: Get AI-generated response\n",
        "        ai_response = get_ai_generated_solution(user_query, filtered_data)\n",
        "\n",
        "        # Step 5: Display Results\n",
        "        st.write(\"### Recommended Solution:\")\n",
        "        if dataset_solution:\n",
        "            st.write(dataset_solution)\n",
        "        else:\n",
        "            st.write(\"No exact match found in the dataset.\")\n",
        "\n",
        "        st.write(\"### Detailed Solution:\")\n",
        "        st.write(ai_response)\n",
        "\n",
        "    else:\n",
        "        st.warning(\"Please enter a query.\")\n",
        " the majour working of this pro when an user enter a query\n",
        "Using OpenAI determine the best category in the dataset then to Find the Best Solution Using OpenAI Embeddings\n",
        "then find out closest match using cosine similarity.as a final result Returns the best solution based on similarity and generating a Solution Using GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NETWORK INTRUSION DETECTION"
      ],
      "metadata": {
        "id": "N7rvUQoxxfGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt\"\n",
        "    columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\",\n",
        "               \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
        "               \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
        "               \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\",\n",
        "               \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
        "               \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\",\n",
        "               \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
        "               \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
        "               \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
        "    data = pd.read_csv(url, names=columns, header=None)\n",
        "    return data\n",
        "\n",
        "def convert_label(x):\n",
        "    if isinstance(x, str):\n",
        "        return 0 if x.strip().lower().startswith('normal') else 1\n",
        "    return x\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(data):\n",
        "    categorical_cols = ['protocol_type', 'service', 'flag']\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "    data['label'] = data['label'].apply(convert_label)\n",
        "    data = data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    data = data.dropna(axis=1, how='all')\n",
        "\n",
        "    X = data.drop(columns=['label'])\n",
        "    y = data['label'].values\n",
        "\n",
        "    imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X = imputer.fit_transform(X)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Build ANN model\n",
        "def build_ann(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build CNN model\n",
        "def build_cnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        Flatten(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build RNN model (LSTM)\n",
        "def build_rnn(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
        "        LSTM(50),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"🔐 Intrusion Detection System using ML & Deep Learning\")\n",
        "st.sidebar.header(\"Model Selection\")\n",
        "\n",
        "# Load and preprocess\n",
        "with st.spinner(\"Loading data and preprocessing...\"):\n",
        "    data = load_data()\n",
        "    X, y = preprocess_data(data)\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "    for train_index, test_index in sss.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "# Model choices\n",
        "model_choice = st.sidebar.selectbox(\"Choose a model\", [\n",
        "    \"Random Forest\", \"SVM\", \"Naïve Bayes\", \"KNN\", \"Logistic Regression\", \"ANN\", \"CNN\", \"RNN\"\n",
        "])\n",
        "\n",
        "if model_choice in [\"Random Forest\", \"SVM\", \"Naïve Bayes\", \"KNN\", \"Logistic Regression\"]:\n",
        "    models = {\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"SVM\": SVC(kernel='linear'),\n",
        "        \"Naïve Bayes\": GaussianNB(),\n",
        "        \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
        "    }\n",
        "\n",
        "    model = models[model_choice]\n",
        "    with st.spinner(f\"Training {model_choice}...\"):\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    st.subheader(f\"{model_choice} Results\")\n",
        "    st.write(f\"**Accuracy:** {acc:.4f}\")\n",
        "    st.text(\"Classification Report:\")\n",
        "    st.text(classification_report(y_test, y_pred))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "elif model_choice == \"ANN\":\n",
        "    with st.spinner(\"Training ANN...\"):\n",
        "        ann = build_ann(X_train.shape[1])\n",
        "        ann.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "        acc = ann.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    st.subheader(\"ANN Results\")\n",
        "    st.write(f\"**Accuracy:** {acc:.4f}\")\n",
        "\n",
        "elif model_choice == \"CNN\":\n",
        "    X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "    with st.spinner(\"Training CNN...\"):\n",
        "        cnn = build_cnn((X_train.shape[1], 1))\n",
        "        cnn.fit(X_train_cnn, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "        acc = cnn.evaluate(X_test_cnn, y_test, verbose=0)[1]\n",
        "    st.subheader(\"CNN Results\")\n",
        "    st.write(f\"**Accuracy:** {acc:.4f}\")\n",
        "\n",
        "elif model_choice == \"RNN\":\n",
        "    X_train_rnn = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test_rnn = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "    with st.spinner(\"Training RNN...\"):\n",
        "        rnn = build_rnn((X_train.shape[1], 1))\n",
        "        rnn.fit(X_train_rnn, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "        acc = rnn.evaluate(X_test_rnn, y_test, verbose=0)[1]\n",
        "    st.subheader(\"RNN Results\")\n",
        "    st.write(f\"**Accuracy:** {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "AeBRJWqzxqKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FRAUD DETECTION"
      ],
      "metadata": {
        "id": "O915ZvRQxm4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "def load_data():\n",
        "    file_path = r\"G:\\\\vs\\\\Telecome\\\\fraud_detection\\\\CDR-Call-Details.csv\"  # Ensure the file is in the same directory\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "df = load_data()\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    features = ['Day Calls', 'Day Mins', 'Intl Calls', 'Intl Mins', 'CustServ Calls']\n",
        "    X = df[features]\n",
        "    y = df['isFraud'].astype(int)  # Convert boolean to int\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "\n",
        "# Train Model\n",
        "def train_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "model = train_model(X_train, y_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    return acc, report\n",
        "\n",
        "accuracy, report = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"Fraud Detection in SIM Card Usage\")\n",
        "st.write(\"Detecting fraudulent SIM cloning and unusual call behaviors using machine learning.\")\n",
        "\n",
        "# Display Dataset\n",
        "if st.checkbox(\"Show Raw Data\"):\n",
        "    st.dataframe(df.head())\n",
        "\n",
        "# Display Fraud Statistics\n",
        "st.subheader(\"Fraud Statistics\")\n",
        "st.write(f\"Fraudulent Cases: {df['isFraud'].sum()} out of {len(df)} records\")\n",
        "\n",
        "# Accuracy & Report\n",
        "st.subheader(\"Model Performance\")\n",
        "st.write(f\"**Accuracy:** {accuracy:.2f}\")\n",
        "st.text(\"Classification Report:\")\n",
        "st.text(report)\n",
        "\n",
        "# Fraud Visualization\n",
        "st.subheader(\"Fraud Distribution\")\n",
        "fig, ax = plt.subplots()\n",
        "sns.countplot(x=df['isFraud'], palette='coolwarm', ax=ax)\n",
        "st.pyplot(fig)\n",
        "\n",
        "# User Input for Prediction\n",
        "st.subheader(\"Fraud Prediction Tool\")\n",
        "day_calls = st.number_input(\"Day Calls\", min_value=0, value=30)\n",
        "day_mins = st.number_input(\"Day Minutes\", min_value=0.0, value=180.0)\n",
        "intl_calls = st.number_input(\"International Calls\", min_value=0, value=2)\n",
        "intl_mins = st.number_input(\"International Minutes\", min_value=0.0, value=10.0)\n",
        "cust_serv_calls = st.number_input(\"Customer Service Calls\", min_value=0, value=1)\n",
        "\n",
        "if st.button(\"Predict Fraud\"):\n",
        "    input_data = np.array([[day_calls, day_mins, intl_calls, intl_mins, cust_serv_calls]])\n",
        "    prediction = model.predict(input_data)[0]\n",
        "    st.write(\"**Fraud Detected! 🚨**\" if prediction == 1 else \"**No Fraud Detected. ✅**\")\n"
      ],
      "metadata": {
        "id": "Dvrwc6jPxgSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TELECOME OPTIMIZATION"
      ],
      "metadata": {
        "id": "QxngAvg1xntY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import streamlit as st\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"G:\\\\vs\\\\Telecome\\\\telecom_network_optimization.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "features = ['Active_Users', 'Traffic_Load_MBps', 'Call_Drop_Rate', 'Latency_ms', 'Previous_Failures', 'Tower_Health_Score']\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[features])\n",
        "\n",
        "# Step 2: K-Means Clustering for Congestion Detection\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "df['Congestion_Cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "# Step 3: Load Balancing with Reinforcement Learning\n",
        "class LoadBalancer:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9):\n",
        "        self.q_table = {}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def get_action(self, state):\n",
        "        return np.argmax(self.q_table.get(state, np.zeros(2)))\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        old_value = self.q_table.get(state, np.zeros(2))[action]\n",
        "        next_max = np.max(self.q_table.get(next_state, np.zeros(2)))\n",
        "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "        self.q_table.setdefault(state, np.zeros(2))[action] = new_value\n",
        "\n",
        "load_balancer = LoadBalancer()\n",
        "df['Balanced_Load'] = df['Traffic_Load_MBps'].apply(lambda x: x * 0.8 if load_balancer.get_action(x) == 1 else x)\n",
        "\n",
        "# Step 4: Failure Risk Prediction - Machine Learning Model (Random Forest)\n",
        "X = df[features]\n",
        "y = df['Failure_Risk']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_report = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# Step 5: Failure Risk Prediction - Deep Learning Model\n",
        "nn_model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 6: Predictive Maintenance Strategy\n",
        "failure_threshold = df['Tower_Health_Score'].quantile(0.2)\n",
        "df['Maintenance_Flag'] = df['Tower_Health_Score'] < failure_threshold\n",
        "\n",
        "# Streamlit Dashboard\n",
        "st.title(\"Telecom Network Optimization Dashboard\")\n",
        "\n",
        "st.subheader(\"Traffic Load Distribution\")\n",
        "st.bar_chart(df[\"Traffic_Load_MBps\"])\n",
        "\n",
        "st.subheader(\"Latency vs. Traffic Load\")\n",
        "st.scatter_chart(df, x=\"Traffic_Load_MBps\", y=\"Latency_ms\", color=\"Congestion_Cluster\")\n",
        "\n",
        "st.subheader(\"Tower Health Score Distribution\")\n",
        "st.bar_chart(df[\"Tower_Health_Score\"])\n",
        "\n",
        "st.subheader(\"Failure Risk Prediction - Random Forest\")\n",
        "st.text(f\"Random Forest Accuracy: {rf_accuracy:.2f}\")\n",
        "st.text(\"Classification Report:\")\n",
        "st.text(rf_report)\n",
        "\n",
        "st.subheader(\"Predictive Maintenance\")\n",
        "st.dataframe(df[['Tower_Health_Score', 'Maintenance_Flag']])\n"
      ],
      "metadata": {
        "id": "o1Fe5_NwxoV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TELECOME SEGMENTATION"
      ],
      "metadata": {
        "id": "Mnadt9Hb2umd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import streamlit as st\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from io import BytesIO\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"📊 Telecom Customer Segmentation\")\n",
        "\n",
        "# Sidebar for User Inputs\n",
        "st.sidebar.header(\"🔧 Settings\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"📂 Upload Telecom Customer Data (CSV)\", type=[\"csv\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "\n",
        "    # Check for required columns\n",
        "    required_columns = {\"Monthly Spend ($)\", \"Data Usage (GB)\", \"Voice Minutes Used\", \"SMS Sent\"}\n",
        "    if not required_columns.issubset(df.columns):\n",
        "        st.error(f\"⚠️ The dataset must contain the following columns: {required_columns}\")\n",
        "    else:\n",
        "        # Drop missing values\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        # Select relevant features\n",
        "        features = list(required_columns)\n",
        "        X = df[features]\n",
        "\n",
        "        # Standardize the data\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # User input for number of clusters\n",
        "        n_clusters = st.sidebar.slider(\"🔢 Select Number of Clusters\", min_value=2, max_value=10, value=3, step=1)\n",
        "\n",
        "        # Apply K-Means Clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # Define dynamic bundle recommendations\n",
        "        bundle_mapping = {i: f\"Plan {i+1}\" for i in range(n_clusters)}\n",
        "        df[\"Recommended Bundle\"] = df[\"Cluster\"].map(bundle_mapping)\n",
        "\n",
        "        # Display the segmented dataset\n",
        "        st.write(\"### 📋 Segmented Customer Data\")\n",
        "        st.dataframe(df.head(10))\n",
        "\n",
        "        # Cluster Insights\n",
        "        st.write(\"### 📊 Cluster Insights\")\n",
        "        cluster_summary = df.groupby(\"Cluster\")[features].mean().round(2)\n",
        "        st.dataframe(cluster_summary)\n",
        "\n",
        "        # Visualization of Clusters\n",
        "        st.write(\"### 📊 Customer Segmentation Visualization\")\n",
        "\n",
        "        # Select feature pair for visualization\n",
        "        feature_x = st.selectbox(\"Select X-axis Feature\", features, index=0)\n",
        "        feature_y = st.selectbox(\"Select Y-axis Feature\", features, index=1)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.scatterplot(x=df[feature_x], y=df[feature_y], hue=df[\"Cluster\"], palette=\"viridis\", ax=ax)\n",
        "        ax.set_xlabel(feature_x)\n",
        "        ax.set_ylabel(feature_y)\n",
        "        ax.set_title(f\"Customer Segmentation: {feature_x} vs {feature_y}\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Download segmented data\n",
        "        output = BytesIO()\n",
        "        df.to_csv(output, index=False)\n",
        "        output.seek(0)\n",
        "\n",
        "        st.download_button(\n",
        "            label=\"📥 Download Segmented Data\",\n",
        "            data=output,\n",
        "            file_name=\"telecom_customer_segmentation.csv\",\n",
        "            mime=\"text/csv\"\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "tV42tefq2xRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}